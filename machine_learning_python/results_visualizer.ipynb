{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1fa2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muckelroyiii/Desktop/RISS_Research/riss_experimental_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(sys.path[0] + \"/..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as rf\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import scipy.io\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.neighbors import KDTree\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "from data_preparation import data_preparation\n",
    "from loaders.rad_cube_loader import RADCUBE_DATASET_TIME\n",
    "from networks.network_time import NeuralNetworkRadarDetector\n",
    "from utils.compute_metrics import compute_metrics_time, compute_pd_pfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b45c1",
   "metadata": {},
   "source": [
    "# Generate Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236ff0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CUDA initialization\n",
    "if torch.cuda.is_available():\n",
    "    # torch.cuda.init()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "params = data_preparation.get_default_params()\n",
    "\n",
    "# Initialise parameters\n",
    "params[\"dataset_path\"] = '/media/muckelroyiii/ExtremePro/RaDelft'\n",
    "params[\"train_val_scenes\"] = [1, 3]\n",
    "params[\"test_scenes\"] = [2, 6]\n",
    "params[\"train_test_split_percent\"] = 0.8\n",
    "params[\"use_npy_cubes\"] = False\n",
    "params[\"cfar_folder\"] = 'radar_ososos'\n",
    "params['label_smoothing'] = False\n",
    "params[\"quantile\"] = False\n",
    "\n",
    "# This must be kept to false. If the network without elevation is needed, use network_noElevation.py instead\n",
    "params[\"bev\"] = False\n",
    "\n",
    "checkpoint_paths = {\n",
    "    '/home/muckelroyiii/Desktop/RISS_Research/checkpoints-resnet18/model-epoch=19-val_loss=0.0004.ckpt',\n",
    "    '/home/muckelroyiii/Desktop/RISS_Research/checkpoints-resnet50/model-epoch=15-val_loss=0.0004.ckpt',\n",
    "    '/home/muckelroyiii/Desktop/RISS_Research/checkpoints-resnet152/model-epoch=17-val_loss=0.0009.ckpt',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f86494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name(checkpoint_path):\n",
    "    # Look for pattern like \"checkpoints-resnet18\"\n",
    "    match = re.search(r'checkpoints-([^/]+)', checkpoint_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 'unknown_model'\n",
    "\n",
    "def generate_point_clouds(params, checkpoints, print_path=False, overwrite_pc=False):\n",
    "    # Check for GPU availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    eval_modes = ['train', 'test', 'validation']\n",
    "\n",
    "    # Generate PCs based on each model checkpoint path\n",
    "    for checkpoint in checkpoints:\n",
    "\n",
    "        # Grab the model name\n",
    "        model_name = extract_model_name(checkpoint)\n",
    "        \n",
    "        # Load Model\n",
    "        try: \n",
    "            cp = torch.load(checkpoint, map_location=device) # Load checkpoint to device (GPU)\n",
    "            model = NeuralNetworkRadarDetector('FPN', model_name, params, in_channels=64, out_classes=34)\n",
    "            model.load_state_dict(cp['state_dict'])\n",
    "        except Exception as e:\n",
    "            print(f'Error loading model ({model_name}) from checkpoint {checkpoint}: {e}')\n",
    "            continue\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        for mode in eval_modes:\n",
    "            # Construct Data Loader\n",
    "            dataset = RADCUBE_DATASET_TIME(mode=mode, params=params)\n",
    "            loader = DataLoader(dataset, batch_size=2, shuffle=False, num_workers=5, pin_memory=False, prefetch_factor=1)\n",
    "\n",
    "            # Create base directory structure\n",
    "            base_network_path = f'network/{model_name}/{mode}/'\n",
    "            file_skip_counter = 0\n",
    "\n",
    "            # Actual Generation of the point clouds\n",
    "            for batch_idx, batch in tqdm(enumerate(loader), desc=f'generating point clouds for {model_name}: {mode}', unit='batch(s)'):\n",
    "                radar_cube, lidar_cube, data_dict = batch\n",
    "\n",
    "                # Move data to GPU\n",
    "                radar_cube = radar_cube.to(device, non_blocking=True)\n",
    "                lidar_cube = lidar_cube.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(radar_cube)\n",
    "\n",
    "                    # Move output back to CPU for post-processing\n",
    "                    output_cpu = output.cpu()\n",
    "                    radar_cube_cpu = radar_cube.cpu()\n",
    "\n",
    "                    for i in range(lidar_cube.shape[0]):\n",
    "                        for t in range(lidar_cube.shape[1]):\n",
    "                            output_t = output_cpu[i, t, :, :, :]\n",
    "                            data_dict_t = data_dict[t]\n",
    "\n",
    "                            # Construct save path:\n",
    "                            cfar_path = data_dict_t['cfar_path'][i]\n",
    "                            save_path = re.sub(r\"radar_.+/\", rf'{base_network_path}', cfar_path)\n",
    "                            if os.path.exists(save_path) and not overwrite_pc:\n",
    "                                file_skip_counter += 1\n",
    "                                continue\n",
    "                            print(save_path) if print_path and batch_idx == 1 else None\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "                            radar_pc = data_preparation.cube_to_pointcloud(\n",
    "                                output_t, params, radar_cube_cpu[i, t, :, :, :], 'radar'\n",
    "                            )\n",
    "                            radar_pc[:, 2] = -radar_pc[:, 2]\n",
    "\n",
    "                            # save result\n",
    "                            np.save(save_path, radar_pc)\n",
    "            print(f'PC Overwriting set to {overwrite_pc}, skipped overwriting {file_skip_counter} files...')\n",
    "        # Free up GPU Memory\n",
    "        del model, cp\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_point_clouds(params, checkpoint_paths, print_path=True, overwrite_pc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925ac38",
   "metadata": {},
   "source": [
    "# Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1706ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix(roll, pitch, yaw, translation_vector):\n",
    "    \"\"\"\n",
    "    Create a transformation matrix from roll, pitch, yaw angles (in degrees) and a translation vector.\n",
    "\n",
    "    Args:\n",
    "    roll (float): Roll angle in degrees.\n",
    "    pitch (float): Pitch angle in degrees.\n",
    "    yaw (float): Yaw angle in degrees.\n",
    "    translation_vector (list): A list of three elements representing the translation vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 4x4 transformation matrix.\n",
    "    \"\"\"\n",
    "    # Convert angles from degrees to radians\n",
    "    roll_rad = np.radians(roll)\n",
    "    pitch_rad = np.radians(pitch)\n",
    "    yaw_rad = np.radians(yaw)\n",
    "\n",
    "    # Create individual rotation matrices\n",
    "    R_x = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(roll_rad), -np.sin(roll_rad)],\n",
    "                    [0, np.sin(roll_rad), np.cos(roll_rad)]])\n",
    "\n",
    "    R_y = np.array([[np.cos(pitch_rad), 0, np.sin(pitch_rad)],\n",
    "                    [0, 1, 0],\n",
    "                    [-np.sin(pitch_rad), 0, np.cos(pitch_rad)]])\n",
    "\n",
    "    R_z = np.array([[np.cos(yaw_rad), -np.sin(yaw_rad), 0],\n",
    "                    [np.sin(yaw_rad), np.cos(yaw_rad), 0],\n",
    "                    [0, 0, 1]])\n",
    "\n",
    "    # Combined rotation matrix\n",
    "    R = np.dot(R_z, np.dot(R_y, R_x))\n",
    "\n",
    "    # rotate around y axis again h degrees\n",
    "    h = 1\n",
    "    h_rad = np.radians(h)\n",
    "    R_h = np.array([[np.cos(h_rad), 0, np.sin(h_rad)],\n",
    "                    [0, 1, 0],\n",
    "                    [-np.sin(h_rad), 0, np.cos(h_rad)]])\n",
    "\n",
    "    R = np.dot(R_h, R)\n",
    "    # Create the transformation matrix\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = R\n",
    "    transformation_matrix[:3, 3] = translation_vector\n",
    "\n",
    "    return transformation_matrix\n",
    "\n",
    "\n",
    "def get_sensor_transforms():\n",
    "    calibration_file = \"./utils/calib.txt\"\n",
    "    with open(calibration_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        intrinsic = np.array(lines[2].strip().split(' ')[1:], dtype=np.float32).reshape(3, 4)  # Intrinsics\n",
    "        extrinsic = np.array(lines[5].strip().split(' ')[1:], dtype=np.float32).reshape(3, 4)  # Extrinsic\n",
    "        extrinsic = np.concatenate([extrinsic, [[0, 0, 0, 1]]], axis=0)\n",
    "\n",
    "    camera_projection_matrix, T_camera_lidar = intrinsic, extrinsic\n",
    "\n",
    "    return camera_projection_matrix, T_camera_lidar\n",
    "\n",
    "\n",
    "def homogeneous_transformation(points: np.ndarray, transform: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "This function applies the homogenous transform using the dot product.\n",
    "    :param points: Points to be transformed in a Nx4 numpy array.\n",
    "    :param transform: 4x4 transformation matrix in a numpy array.\n",
    "    :return: Transformed points of shape Nx4 in a numpy array.\n",
    "    \"\"\"\n",
    "    if transform.shape != (4, 4):\n",
    "        raise ValueError(f\"{transform.shape} must be 4x4!\")\n",
    "    if points.shape[1] != 4:\n",
    "        raise ValueError(f\"{points.shape[1]} must be Nx4!\")\n",
    "    return transform.dot(points.T).T\n",
    "\n",
    "\n",
    "def project_3d_to_2d(points: np.ndarray, projection_matrix: np.ndarray):\n",
    "    \"\"\"\n",
    "This function projects the input 3d ndarray to a 2d ndarray, given a projection matrix.\n",
    "    :param points: Homogenous points to be projected.\n",
    "    :param projection_matrix: 4x4 projection matrix.\n",
    "    :return: 2d ndarray of the projected points.\n",
    "    \"\"\"\n",
    "    if points.shape[-1] != 4:\n",
    "        raise ValueError(f\"{points.shape[-1]} must be 4!\")\n",
    "\n",
    "    uvw = projection_matrix.dot(points.T)\n",
    "    uvw /= uvw[2]\n",
    "    uvs = uvw[:2].T\n",
    "    uvs = np.round(uvs).astype(int)\n",
    "\n",
    "    return uvs\n",
    "\n",
    "\n",
    "def project_pcl_to_image(point_cloud, t_camera_pcl, camera_projection_matrix, image_shape):\n",
    "    \"\"\"\n",
    "A helper function which projects a point clouds specific to the dataset to the camera image frame.\n",
    "    :param point_cloud: Point cloud to be projected.\n",
    "    :param t_camera_pcl: Transformation from the pcl frame to the camera frame.\n",
    "    :param camera_projection_matrix: The 4x4 camera projection matrix.\n",
    "    :param image_shape: Size of the camera image.\n",
    "    :return: Projected points, and the depth of each point.\n",
    "    \"\"\"\n",
    "    point_homo = np.hstack((point_cloud[:, :3], np.ones((point_cloud.shape[0], 1))))\n",
    "\n",
    "    radar_points_camera_frame = homogeneous_transformation(point_homo,\n",
    "                                                           transform=t_camera_pcl)\n",
    "\n",
    "    point_depth = radar_points_camera_frame[:, 2]\n",
    "\n",
    "    uvs = project_3d_to_2d(points=radar_points_camera_frame,\n",
    "                           projection_matrix=camera_projection_matrix)\n",
    "\n",
    "    filtered_idx = canvas_crop(points=uvs,\n",
    "                               image_size=image_shape,\n",
    "                               points_depth=point_depth)\n",
    "\n",
    "    # uvs = uvs[filtered_idx]\n",
    "    # point_depth = point_depth[filtered_idx]\n",
    "\n",
    "    return uvs, point_depth, filtered_idx\n",
    "\n",
    "\n",
    "def canvas_crop(points, image_size, points_depth=None):\n",
    "    \"\"\"\n",
    "This function filters points that lie outside a given frame size.\n",
    "    :param points: Input points to be filtered.\n",
    "    :param image_size: Size of the frame.\n",
    "    :param points_depth: Filters also depths smaller than 0.\n",
    "    :return: Filtered points.\n",
    "    \"\"\"\n",
    "    idx = points[:, 0] > 0\n",
    "    idx = np.logical_and(idx, points[:, 0] < image_size[1])\n",
    "    idx = np.logical_and(idx, points[:, 1] > 0)\n",
    "    idx = np.logical_and(idx, points[:, 1] < image_size[0])\n",
    "    if points_depth is not None:\n",
    "        idx = np.logical_and(idx, points_depth > 0)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "# function that takes axis and 2d points and plots them\n",
    "def plot_points_on_image(ax, cax, points_2d, image, point_depths=None, size=40, color_by='depth', alpha=0.1):\n",
    "    ax.imshow(image)\n",
    "    if color_by == \"depth\" and point_depths is not None:\n",
    "        # print(\"color by depth\")\n",
    "        order = np.argsort(-point_depths)\n",
    "        point_depths = point_depths[order]\n",
    "        points_2d = points_2d[order, :]\n",
    "\n",
    "        im = ax.scatter(points_2d[:, 0], points_2d[:, 1], c=point_depths, s=size, alpha=alpha)\n",
    "        cb = ax.figure.colorbar(im, cax=cax)\n",
    "        cb.solids.set(alpha=1)\n",
    "        cb.set_label('Range (m)')\n",
    "    elif color_by == \"height\":\n",
    "        print(\"color by height\")\n",
    "        ax.scatter(points_2d[:, 0], points_2d[:, 1], c=points_2d[:, 1], s=size)\n",
    "\n",
    "    elif color_by == \"depth_scale\":\n",
    "        print(\"color by depth scale\")\n",
    "        ax.scatter(points_2d[:, 0], points_2d[:, 1], c=point_depths, s=3 / point_depths * point_depths)\n",
    "        # scale dots based on depth\n",
    "\n",
    "    ax.axis('off')\n",
    "    return cb\n",
    "\n",
    "\n",
    "# filter by distance\n",
    "def filter_by_distance(point_depths, threshold):\n",
    "    \"\"\"\n",
    "This function filters points that are further than a given distance.\n",
    "    :param points: Input points to be filtered.\n",
    "    :param distance: Distance threshold.\n",
    "    :return: Filtered points.\n",
    "    \"\"\"\n",
    "    return point_depths < threshold\n",
    "\n",
    "\n",
    "def filter_point_cloud(point_cloud):\n",
    "    # remove behind sensor:\n",
    "    point_cloud = point_cloud[point_cloud[:, 0] > 0, :]\n",
    "\n",
    "    # remove to far in x\n",
    "    point_cloud = point_cloud[point_cloud[:, 0] < 30, :]\n",
    "\n",
    "    # remove to far in y\n",
    "    point_cloud = point_cloud[point_cloud[:, 1] < 10, :]\n",
    "    point_cloud = point_cloud[point_cloud[:, 1] > -10, :]\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "def plot_bev(pointcloud, ax, cax, cblabel=True):\n",
    "    \"\"\"\n",
    "This function plots the BEV of a point cloud.\n",
    "    :param pointcloud: Input point cloud.\n",
    "    :param ax: Matplotlib axis.\n",
    "    :param cax: Colorbar Axis.\n",
    "    \"\"\"\n",
    "    # colored by height\n",
    "    im = ax.scatter(-pointcloud[:, 1], pointcloud[:, 0], c=pointcloud[:, 2], s=0.1)\n",
    "    ax.set_xlim(-16, 16)\n",
    "    ax.set_ylim(5, 30)\n",
    "    ax.set_xlabel('y [m]')\n",
    "    ax.set_xlabel('y [m]')\n",
    "    ax.set_ylabel('x [m]')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cb = ax.figure.colorbar(im, cax=cax)\n",
    "\n",
    "    if cblabel == True:\n",
    "        cb.set_label('Height (m)')\n",
    "\n",
    "    return cb\n",
    "\n",
    "# From the paper\n",
    "def compute_chamfer_distance(point_cloud1, point_cloud2):\n",
    "    \"\"\"\n",
    "    Compute the Chamfer distance between two set of points\n",
    "\n",
    "    :param point_cloud1: the first set of points\n",
    "    :param point_cloud2: the second set of points\n",
    "    :return: the Chamfer distance\n",
    "    \"\"\"\n",
    "    tree1 = KDTree(point_cloud1, metric='euclidean')\n",
    "    tree2 = KDTree(point_cloud2, metric='euclidean')\n",
    "    distances1, _ = tree1.query(point_cloud2)\n",
    "    distances2, _ = tree2.query(point_cloud1)\n",
    "    av_dist1 = np.sum(distances1) / np.size(distances1)\n",
    "    av_dist2 = np.sum(distances2) / np.size(distances2)\n",
    "    dist = av_dist1 + av_dist2\n",
    "\n",
    "\n",
    "    return dist\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945bf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamfer distance function\n",
    "def compute_chamfer_distance(point_cloud1, point_cloud2):\n",
    "    \"\"\"\n",
    "    Compute the Chamfer distance between two set of points\n",
    "\n",
    "    :param point_cloud1: the first set of points\n",
    "    :param point_cloud2: the second set of points\n",
    "    :return: the Chamfer distance\n",
    "    \"\"\"\n",
    "    if len(point_cloud1) == 0 or len(point_cloud2) == 0:\n",
    "        return float('inf')  # Return infinity if either point cloud is empty\n",
    "        \n",
    "    tree1 = KDTree(point_cloud1, metric='euclidean')\n",
    "    tree2 = KDTree(point_cloud2, metric='euclidean')\n",
    "    distances1, _ = tree1.query(point_cloud2)\n",
    "    distances2, _ = tree2.query(point_cloud1)\n",
    "    av_dist1 = np.sum(distances1) / np.size(distances1)\n",
    "    av_dist2 = np.sum(distances2) / np.size(distances2)\n",
    "    dist = av_dist1 + av_dist2\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8840b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "train dataset loaded with 3078 samples\n",
      "scenes used: [1, 3]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "test dataset loaded with 2719 samples\n",
      "scenes used: [2, 6]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "val dataset loaded with 342 samples\n",
      "scenes used: [1, 3]\n",
      "--------------------------------------------------\n",
      "Grabbing Valid Data...\n"
     ]
    }
   ],
   "source": [
    "camera_projection_matrix, T_camera_lidar = get_sensor_transforms()\n",
    "roll, pitch, yaw = 0, -85, 90 # Roll, pitch, and yaw angles in degrees\n",
    "translation_vector = [0.195, 0.207, -0.482] # T_camera_lidar[:3, 3]\n",
    "\n",
    "# Create the transformation matrix\n",
    "transformation_matrix = create_transformation_matrix(roll, pitch, yaw, translation_vector)\n",
    "# print(transformation_matrix)\n",
    "# print(T_camera_lidar)\n",
    "distance_from_camera = 30 # 30 what?\n",
    "\n",
    "# Video Writer\n",
    "Writer = FFMpegWriter(fps=10, metadata=dict(artist='muckelroyiii'), bitrate=1800)\n",
    "\n",
    "train_dataset = RADCUBE_DATASET_TIME(mode='train', params=params)\n",
    "test_dataset = RADCUBE_DATASET_TIME(mode='test', params=params)\n",
    "val_dataset = RADCUBE_DATASET_TIME(mode='val', params=params)\n",
    "model_names = ['resnet18', 'resnet50', 'resnet101', 'resnet152']\n",
    "# model_names = ['resnet152']\n",
    "datasets = [('train', train_dataset), ('test', test_dataset), ('val', val_dataset)]\n",
    "\n",
    "# Grab data\n",
    "print(\"Grabbing Valid Data...\")\n",
    "valid_data = {}\n",
    "for model_name in model_names:\n",
    "    for mode, dataset in datasets:\n",
    "        key = f'{model_name}-{mode}'\n",
    "        valid_data[key] = []\n",
    "\n",
    "        # Grab data for the model-mode combination\n",
    "        for outer_dict in dataset.data_dict.values():\n",
    "            for ii, paths_dict in outer_dict.items():\n",
    "                if ii in range(1, 325):\n",
    "                    # print(paths_dict)\n",
    "                    cam = paths_dict[\"cam_path\"]\n",
    "                    lidar = paths_dict[\"gt_path\"]\n",
    "                    # lidar = lidar.replace(\"rslidar_points_clean\", \"rslidar_points\")\n",
    "                    cfar = paths_dict[\"cfar_path\"]\n",
    "\n",
    "                    # Create Radar path for specific model and mode\n",
    "                    radar = re.sub(r\"radar_.+/\", f\"network/{model_name}/{mode}/\", cfar)\n",
    "                    if os.path.isfile(radar):\n",
    "                        valid_data[key].append((cam, lidar, cfar, radar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85801ee",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50cbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_2D(nFrames: int, model_mode_key: str, valid_data: dict) -> list:\n",
    "    '''\n",
    "    This function takes the required number of frames to read and returns\n",
    "    a list of 2D matrices that represents Lidar 2D point-clouds projection\n",
    "    \n",
    "    Parameters:\n",
    "    nFrames: number of frames\n",
    "    model_mode_key: key like \"resnet18-val\" to access specific model-mode data\n",
    "    valid_data: dictionary containing valid data paths for each model-mode\n",
    "    \n",
    "    Returns:\n",
    "    frames_lidar: list of 2D matrices\n",
    "    frames_lidar_3D: list of 3D points\n",
    "    '''\n",
    "    frames_lidar = []\n",
    "    frames_lidar_3D = []\n",
    "    \n",
    "    # Get the data for this specific model-mode combination\n",
    "    data_list = valid_data.get(model_mode_key, [])\n",
    "    \n",
    "    for ii, (cam, lidar, cfar, radar) in enumerate(data_list):\n",
    "        if ii == 0:\n",
    "            img = plt.imread(cam)\n",
    "        \n",
    "        pointcloud = np.load(lidar)\n",
    "        pointcloud = rf.structured_to_unstructured(pointcloud)\n",
    "        pointcloud = pointcloud.reshape(-1, 4)\n",
    "\n",
    "        uvs, point_depths, filtered_idx = project_pcl_to_image(pointcloud, transformation_matrix, camera_projection_matrix, (1216, 1936))\n",
    "        filtered_by_distance_idx = filter_by_distance(point_depths, distance_from_camera)\n",
    "        filtered_idx = np.logical_and(filtered_idx, filtered_by_distance_idx)\n",
    "        uvs = uvs[filtered_idx]\n",
    "\n",
    "        mypoints = point_depths[filtered_idx] # this is depth bins\n",
    "        order = np.argsort(-mypoints)\n",
    "        mypoints = mypoints[order]\n",
    "        mypoints_2d = uvs[order, :]\n",
    "\n",
    "        matrix_lidar_temp = np.zeros((np.shape(img)[0], np.shape(img)[1]))\n",
    "\n",
    "        for jj, points in enumerate(mypoints_2d):\n",
    "            matrix_lidar_temp[points[1]-1, points[0]-1] = np.exp(100/round(mypoints[jj],2))\n",
    "\n",
    "        frames_lidar.append(matrix_lidar_temp)\n",
    "\n",
    "        # 3D points for chamfer distance calculation:\n",
    "        point_3D = np.concatenate((mypoints_2d[:, [1, 0]], np.round(mypoints[:, np.newaxis],2)), axis=1)\n",
    "        frames_lidar_3D.append(point_3D)\n",
    "        \n",
    "        if ii == (nFrames-1): \n",
    "            break\n",
    "    \n",
    "    return frames_lidar, frames_lidar_3D\n",
    "\n",
    "\n",
    "def get_radar_2D(nFrames: int, model_mode_key: str, valid_data: dict) -> list:\n",
    "    '''\n",
    "    This function takes the required number of frames to read and returns\n",
    "    a list of 2D matrices that represents radar 2D point-clouds projection\n",
    "    \n",
    "    Parameters:\n",
    "    nFrames: number of frames\n",
    "    model_mode_key: key like \"resnet18-val\" to access specific model-mode data\n",
    "    valid_data: dictionary containing valid data paths for each model-mode\n",
    "    \n",
    "    Returns:\n",
    "    frames_radar: list of 2D matrices\n",
    "    frames_radar_3D: list of 3D points\n",
    "    '''\n",
    "    frames_radar = []\n",
    "    frames_radar_3D = []\n",
    "    \n",
    "    # Get the data for this specific model-mode combination\n",
    "    data_list = valid_data.get(model_mode_key, [])\n",
    "    \n",
    "    for ii, (cam, lidar, cfar, radar) in enumerate(data_list):\n",
    "        img = plt.imread(cam)\n",
    "\n",
    "        radar_points = np.load(radar)\n",
    "        radar_points = radar_points[:, :-1]\n",
    "        pointcloud = radar_points.reshape(-1, 3)\n",
    "        pointcloud[:, 1] = -pointcloud[:, 1]\n",
    "\n",
    "        pointcloud = data_preparation.transform_point_cloud(pointcloud, [0, 0, -params['azimuth_offset']], [-params['x_offset'] / 100, -params['y_offset'] / 100, 0])\n",
    "        uvs, point_depths, filtered_idx = project_pcl_to_image(pointcloud, transformation_matrix, camera_projection_matrix, (1216, 1936))\n",
    "        filtered_by_distance_idx = filter_by_distance(point_depths, distance_from_camera)\n",
    "        filtered_idx = np.logical_and(filtered_idx, filtered_by_distance_idx)\n",
    "        uvs = uvs[filtered_idx]\n",
    "\n",
    "        ## Here\n",
    "        mypoints = point_depths[filtered_idx]\n",
    "        order = np.argsort(-mypoints)\n",
    "        mypoints = mypoints[order]\n",
    "        mypoints_2d = uvs[order, :]\n",
    "\n",
    "        matrix_radar_temp = np.zeros((np.shape(img)[0], np.shape(img)[1]))\n",
    "        for jj, points in enumerate(mypoints_2d):\n",
    "            matrix_radar_temp[points[1]-1, points[0]-1] = np.exp(100/round(mypoints[jj],2))\n",
    "\n",
    "        frames_radar.append(matrix_radar_temp)\n",
    "\n",
    "        # 3D points for chamfer distance calculation:\n",
    "        point_3D = np.concatenate((mypoints_2d[:, [1, 0]], np.round(mypoints[:, np.newaxis],2)), axis=1)\n",
    "        frames_radar_3D.append(point_3D)\n",
    "\n",
    "        if ii == (nFrames-1): \n",
    "            break\n",
    "\n",
    "    return frames_radar, frames_radar_3D\n",
    "\n",
    "\n",
    "def get_CFAR_2D(nFrames: int, model_mode_key: str, valid_data: dict) -> list:\n",
    "    '''\n",
    "    This function takes the required number of frames to read and returns\n",
    "    a list of 2D matrices that represents CFAR 2D point-clouds projection\n",
    "    \n",
    "    Parameters:\n",
    "    nFrames: number of frames\n",
    "    model_mode_key: key like \"resnet18-val\" to access specific model-mode data\n",
    "    valid_data: dictionary containing valid data paths for each model-mode\n",
    "    \n",
    "    Returns:\n",
    "    frames_CFAR: list of 2D matrices\n",
    "    frames_CFAR_3D: list of 3D points\n",
    "    '''\n",
    "    frames_CFAR = []\n",
    "    frames_CFAR_3D = []\n",
    "    \n",
    "    # Get the data for this specific model-mode combination\n",
    "    data_list = valid_data.get(model_mode_key, [])\n",
    "    \n",
    "    for ii, (cam, lidar, cfar, radar) in enumerate(data_list):\n",
    "        img = plt.imread(cam)\n",
    "\n",
    "        cfar_points = data_preparation.read_pointcloud(cfar, mode=\"radar\")\n",
    "        pointcloud = cfar_points\n",
    "        pointcloud = data_preparation.transform_point_cloud(pointcloud, [0, 0, -params['azimuth_offset']],\n",
    "                                                            [-params['x_offset'] / 100, -params['y_offset'] / 100, 0])\n",
    "        uvs, point_depths, filtered_idx = project_pcl_to_image(pointcloud, transformation_matrix,\n",
    "                                                                camera_projection_matrix, (1216, 1936))\n",
    "        filtered_by_distance_idx = filter_by_distance(point_depths, distance_from_camera)\n",
    "        filtered_idx = np.logical_and(filtered_idx, filtered_by_distance_idx)\n",
    "        uvs = uvs[filtered_idx]\n",
    "\n",
    "        ## Here\n",
    "        mypoints = point_depths[filtered_idx]\n",
    "        order = np.argsort(-mypoints)\n",
    "        mypoints = mypoints[order]\n",
    "        mypoints_2d = uvs[order, :]\n",
    "\n",
    "        matrix_radar_temp = np.zeros((np.shape(img)[0], np.shape(img)[1]))\n",
    "        for jj, points in enumerate(mypoints_2d):\n",
    "            matrix_radar_temp[points[1]-1, points[0]-1] = np.exp(100/round(mypoints[jj],2))\n",
    "\n",
    "        frames_CFAR.append(matrix_radar_temp)\n",
    "\n",
    "        # 3D points for chamfer distance calculation:\n",
    "        point_3D = np.concatenate((mypoints_2d[:, [1, 0]], np.round(mypoints[:, np.newaxis],2)), axis=1)\n",
    "        frames_CFAR_3D.append(point_3D)\n",
    "\n",
    "        if ii == (nFrames-1): \n",
    "            break\n",
    "            \n",
    "    return frames_CFAR, frames_CFAR_3D\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# frames_lidar, frames_lidar_3D = get_lidar_2D(nFrames=20, model_mode_key=\"resnet18-val\", valid_data=valid_data)\n",
    "# frames_radar, frames_radar_3D = get_radar_2D(nFrames=20, model_mode_key=\"resnet18-val\", valid_data=valid_data)\n",
    "# frames_cfar, frames_cfar_3D = get_CFAR_2D(nFrames=20, model_mode_key=\"resnet18-val\", valid_data=valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15bbee",
   "metadata": {},
   "source": [
    "### Camera Overlay w/ 2D BEV Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee23b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 Figure for plots\n",
    "# fig, axs = plt.subplots(2, 3)\n",
    "# fig.set_size_inches(18.5, 10.5)\n",
    "# plt.show()\n",
    "# plt.close(fig)\n",
    "max_frames = 25\n",
    "\n",
    "# Create videos for all model-mode combinations with chamfer distance\n",
    "def create_videos_for_all_models_with_chamfer(valid_data, max_frames=500):\n",
    "    \"\"\"\n",
    "    Create separate videos for each model-mode combination with chamfer distance calculation\n",
    "    \"\"\"\n",
    "    for model_mode_key, data_list in valid_data.items():\n",
    "        if not data_list:\n",
    "            print(f\"Skipping {model_mode_key} - no data available\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Creating video for {model_mode_key}...\")\n",
    "        \n",
    "        # Store chamfer distances for analysis\n",
    "        chamfer_distances_radar = []\n",
    "        chamfer_distances_cfar = []\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 3)\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        \n",
    "        with Writer.saving(fig, f\"./sensor_visualization_{model_mode_key}.mp4\", 100):\n",
    "            for counter, (cam, lidar, cfar, radar) in tqdm(enumerate(data_list), desc=f\"Video: {model_mode_key}\", unit=\" frames\"):\n",
    "                if counter > max_frames:\n",
    "                    break\n",
    "\n",
    "                axs[0,0].cla()\n",
    "                axs[0,1].cla()\n",
    "                axs[1,0].cla()\n",
    "                axs[1,1].cla()\n",
    "                axs[0,2].cla()\n",
    "                axs[1,2].cla()\n",
    "\n",
    "                divider = make_axes_locatable(axs[0,0])\n",
    "                cax00 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                divider = make_axes_locatable(axs[0,1])\n",
    "                cax01 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                divider = make_axes_locatable(axs[1,0])\n",
    "                cax10 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                divider = make_axes_locatable(axs[1,1])\n",
    "                cax11 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                divider = make_axes_locatable(axs[0,2])\n",
    "                cax02 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                divider = make_axes_locatable(axs[1,2])\n",
    "                cax12 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "                params = data_preparation.get_default_params()\n",
    "\n",
    "                ####### Load and process LiDAR (ground truth) ######\n",
    "                lidarpc = np.load(lidar)\n",
    "                # lidarpc = rf.structured_to_unstructured(lidarpc)\n",
    "                lidarpc = lidarpc.reshape(-1, 3)\n",
    "                lidarpc = data_preparation.transform_point_cloud(\n",
    "                    lidarpc,\n",
    "                    [0, 0, -params['azimuth_offset']],\n",
    "                    [-params['x_offset'] / 100, -params['y_offset'] / 100, 0]\n",
    "                )\n",
    "                lidarpc_bev = lidarpc\n",
    "\n",
    "                \n",
    "                # For chamfer distance, use 3D coordinates only\n",
    "                lidarpc_3d = lidarpc[:, :3].copy()\n",
    "                lidarpc_3d[:, 1] = -lidarpc_3d[:, 1]  # Apply coordinate transformation\n",
    "                uvs_lidar, point_depths_lidar, filtered_idx_lidar = project_pcl_to_image(\n",
    "                    lidarpc, transformation_matrix, camera_projection_matrix, (1216, 1936)\n",
    "                )\n",
    "                filter_by_distance_idx_lidar = filter_by_distance(point_depths_lidar, distance_from_camera)\n",
    "                filtered_idx_lidar = np.logical_and(filtered_idx_lidar, filter_by_distance_idx_lidar)\n",
    "                uvs_lidar = uvs_lidar[filtered_idx_lidar]\n",
    "\n",
    "                # Get filtered LiDAR points for chamfer distance\n",
    "                lidarpc_filtered = lidarpc_3d[filtered_idx_lidar]\n",
    "\n",
    "                ####### Load and process Radar ######\n",
    "                radar_points = np.load(radar)\n",
    "                # Remove speed information if present (keep only x, y, z coordinates)\n",
    "                if radar_points.shape[1] == 4:\n",
    "                    radar_points = radar_points[:, :-1]  # Remove last column (speed)\n",
    "                radarpc = radar_points.reshape(-1, 3)\n",
    "                radarpc[:, 1] = -radarpc[:, 1]\n",
    "                radarpc = data_preparation.transform_point_cloud(\n",
    "                    radarpc,\n",
    "                    [0, 0, -params['azimuth_offset']],\n",
    "                    [-params['x_offset'] / 100, -params['y_offset'] / 100, 0]\n",
    "                )\n",
    "                radarpc_bev = radarpc\n",
    "                uvs_radar, point_depths_radar, filtered_idx_radar = project_pcl_to_image(\n",
    "                    radarpc, transformation_matrix, camera_projection_matrix, (1216, 1936)\n",
    "                )\n",
    "                filtered_by_distance_idx_radar = filter_by_distance(point_depths_radar, distance_from_camera)\n",
    "                filtered_idx_radar = np.logical_and(filtered_idx_radar, filtered_by_distance_idx_radar)\n",
    "                uvs_radar = uvs_radar[filtered_idx_radar]\n",
    "\n",
    "                # Get filtered radar points for chamfer distance\n",
    "                radarpc_filtered = radarpc[filtered_idx_radar]\n",
    "\n",
    "                ####### Load and process CFAR ######\n",
    "                cfar_points = data_preparation.read_pointcloud(cfar, mode=\"radar\")\n",
    "                # Ensure only 3D coordinates (x, y, z) are used for CFAR\n",
    "                cfarpc = cfar_points[:, :3]  # Remove any additional columns beyond x, y, z\n",
    "                cfarpc = data_preparation.transform_point_cloud(\n",
    "                    cfarpc,\n",
    "                    [0, 0, -params[\"azimuth_offset\"]],\n",
    "                    [-params[\"x_offset\"] / 100, -params[\"y_offset\"] / 100, 0]\n",
    "                )\n",
    "                cfarpc_bev = cfarpc\n",
    "                uvs_cfar, point_depths_cfar, filtered_idx_cfar = project_pcl_to_image(\n",
    "                    cfarpc, transformation_matrix, camera_projection_matrix, (1216, 1936)\n",
    "                )\n",
    "                filter_by_distance_idx_cfar = filter_by_distance(point_depths_cfar, distance_from_camera)\n",
    "                filtered_idx_cfar = np.logical_and(filtered_idx_cfar, filter_by_distance_idx_cfar)\n",
    "                uvs_cfar = uvs_cfar[filtered_idx_cfar]\n",
    "\n",
    "                # Get filtered CFAR points for chamfer distance\n",
    "                cfarpc_filtered = cfarpc[filtered_idx_cfar]\n",
    "\n",
    "                ####### Calculate Chamfer Distances ######\n",
    "                # print(f\"Lidar PC: {lidarpc_bev}\")\n",
    "                # print(f\"Radar PC: {radarpc_bev}\")\n",
    "                chamfer_radar = compute_chamfer_distance(lidarpc_bev, radarpc_bev)\n",
    "                # print(chamfer_radar)\n",
    "                chamfer_cfar = compute_chamfer_distance(lidarpc_bev, cfarpc_bev)\n",
    "                \n",
    "                chamfer_distances_radar.append(chamfer_radar)\n",
    "                chamfer_distances_cfar.append(chamfer_cfar)\n",
    "\n",
    "                ####### Plotting ######\n",
    "                img = plt.imread(cam)\n",
    "\n",
    "                # set titles with chamfer distances\n",
    "                axs[1,0].set_title(\"LiDAR (Ground Truth)\")\n",
    "                axs[1,1].set_title(f\"NN Detector ({model_mode_key}) -- Chamfer: {chamfer_radar:.3f}\")\n",
    "                axs[1,2].set_title(f\"CFAR -- Chamfer: {chamfer_cfar:.3f}\")\n",
    "                fig.suptitle(f\"Frame {counter + 1} - {model_mode_key}\", fontsize=16)\n",
    "\n",
    "                # Plot LiDAR\n",
    "                cb00 = plot_points_on_image(axs[0,0], cax00, uvs_lidar, img, point_depths_lidar[filtered_idx_lidar], size=7, color_by=\"depth\")\n",
    "                cb10 = plot_bev(lidarpc[filtered_idx_lidar], axs[1,0], cax10, False)\n",
    "\n",
    "                # Plot Radar\n",
    "                cb01 = plot_points_on_image(\n",
    "                    axs[0, 1], cax01, uvs_radar, img, point_depths=point_depths_radar[filtered_idx_radar], size=20, color_by='depth'\n",
    "                )\n",
    "                cb11 = plot_bev(radarpc[filtered_idx_radar], axs[1, 1], cax11, False)\n",
    "\n",
    "                # Plot CFAR\n",
    "                cb02 = plot_points_on_image(\n",
    "                    axs[0, 2], cax02, uvs_cfar, img, point_depths=point_depths_cfar[filtered_idx_cfar], size=20, color_by='depth', alpha=0.5\n",
    "                )\n",
    "                cb12 = plot_bev(cfarpc[filtered_idx_cfar], axs[1, 2], cax12)\n",
    "\n",
    "                # Update plot\n",
    "                plt.pause(0.01)\n",
    "\n",
    "                # Grab frame for video\n",
    "                Writer.grab_frame()\n",
    "\n",
    "                # clear plot\n",
    "                cb00.remove()\n",
    "                cb01.remove()\n",
    "                cb10.remove()\n",
    "                cb11.remove()\n",
    "                cb02.remove()\n",
    "                cb12.remove()\n",
    "                \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        avg_chamfer_radar = np.mean(chamfer_distances_radar)\n",
    "        avg_chamfer_cfar = np.mean(chamfer_distances_cfar)\n",
    "        \n",
    "        print(f\"Completed video for {model_mode_key}\")\n",
    "        print(f\"Average Chamfer Distance - Radar: {avg_chamfer_radar:.3f}, CFAR: {avg_chamfer_cfar:.3f}\")\n",
    "        print(f\"Radar vs CFAR improvement: {((avg_chamfer_cfar - avg_chamfer_radar) / avg_chamfer_cfar * 100):.1f}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def create_comprehensive_comparison_video(valid_data, max_frames=20):\n",
    "    \"\"\"\n",
    "    Create a single comprehensive video comparing all models and modes\n",
    "    Layout: 3 rows x 5 columns\n",
    "    Row 1: EMPTY | resnet18-train | resnet50-train | resnet101-train | resnet152-train\n",
    "    Row 2: lidar | resnet18-test  | resnet50-test  | resnet101-test  | resnet152-test\n",
    "    Row 3: EMPTY | resnet18-val   | resnet50-val   | resnet101-val   | resnet152-val\n",
    "    \"\"\"\n",
    "    \n",
    "    # Close any existing figures to prevent output\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Define the layout\n",
    "    models = ['resnet18', 'resnet50', 'resnet101', 'resnet152']\n",
    "    modes = ['train', 'test', 'val']\n",
    "    \n",
    "    # Check data availability and find common frames\n",
    "    print(\"Checking data availability...\")\n",
    "    available_combinations = []\n",
    "    min_frames = float('inf')\n",
    "    \n",
    "    for model in models:\n",
    "        for mode in modes:\n",
    "            key = f\"{model}-{mode}\"\n",
    "            if key in valid_data and valid_data[key]:\n",
    "                available_combinations.append(key)\n",
    "                min_frames = min(min_frames, len(valid_data[key]))\n",
    "                print(f\"{key}: {len(valid_data[key])} frames\")\n",
    "            else:\n",
    "                print(f\"{key}: NO DATA AVAILABLE\")\n",
    "    \n",
    "    if min_frames == float('inf'):\n",
    "        print(\"No common data found across models!\")\n",
    "        return\n",
    "    \n",
    "    # Use the minimum available frames across all combinations\n",
    "    actual_frames = min(max_frames, min_frames)\n",
    "    print(f\"Creating video with {actual_frames} frames\")\n",
    "    \n",
    "    # Create 3x5 subplot layout\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(25, 15))\n",
    "    \n",
    "    # Store chamfer distances for analysis\n",
    "    chamfer_results = {combo: [] for combo in available_combinations}\n",
    "    \n",
    "    with Writer.saving(fig, \"./comprehensive_model_comparison2.mp4\", 100):\n",
    "        for frame_idx in tqdm(range(actual_frames), desc=\"Creating comprehensive comparison video\", unit=\" frames\"):\n",
    "            \n",
    "            # Clear all subplots\n",
    "            for i in range(3):\n",
    "                for j in range(5):\n",
    "                    axs[i, j].cla()\n",
    "            \n",
    "            # Create colorbar axes for each subplot that will have plots\n",
    "            cax_dict = {}\n",
    "            for i in range(3):\n",
    "                for j in range(5):\n",
    "                    if not ((i == 0 and j == 0) or (i == 2 and j == 0)):  # Skip empty corners\n",
    "                        divider = make_axes_locatable(axs[i, j])\n",
    "                        cax_dict[(i, j)] = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            \n",
    "            # Set up the layout titles\n",
    "            for col, model in enumerate(models, 1):\n",
    "                axs[0, col].set_title(f\"{model}-train\", fontsize=10, fontweight='bold')\n",
    "                axs[1, col].set_title(f\"{model}-test\", fontsize=10, fontweight='bold')\n",
    "                axs[2, col].set_title(f\"{model}-val\", fontsize=10, fontweight='bold')\n",
    "            \n",
    "            fig.suptitle(f\"Model Comparison - Frame {frame_idx + 1}\", fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Get LiDAR data (ground truth) - we'll use test data as reference\n",
    "            lidar_key = \"resnet18-test\"  # Use any available combination for lidar reference\n",
    "            if lidar_key in valid_data and frame_idx < len(valid_data[lidar_key]):\n",
    "                cam, lidar, cfar, _ = valid_data[lidar_key][frame_idx]\n",
    "                \n",
    "                # Process LiDAR\n",
    "                params = data_preparation.get_default_params()\n",
    "                lidarpc = np.load(lidar)\n",
    "                lidarpc = lidarpc.reshape(-1, 3)\n",
    "                lidarpc = data_preparation.transform_point_cloud(\n",
    "                    lidarpc,\n",
    "                    [0, 0, -params['azimuth_offset']],\n",
    "                    [-params['x_offset'] / 100, -params['y_offset'] / 100, 0]\n",
    "                )\n",
    "                lidarpc_bev = lidarpc\n",
    "                \n",
    "                # Plot LiDAR in middle left (row 1, col 0)\n",
    "                axs[1, 0].set_title(\"LiDAR\\n(Ground Truth)\", fontsize=10, fontweight='bold')\n",
    "                cb_lidar = plot_bev(lidarpc, axs[1, 0], cax_dict.get((1, 0)), False)\n",
    "                \n",
    "                # Load image for reference\n",
    "                img = plt.imread(cam)\n",
    "            \n",
    "            # Store plot objects for removal\n",
    "            plot_objects = []\n",
    "            \n",
    "            # Process each model-mode combination\n",
    "            for row, mode in enumerate(modes):\n",
    "                for col, model in enumerate(models, 1):\n",
    "                    key = f\"{model}-{mode}\"\n",
    "                    \n",
    "                    if key not in valid_data or frame_idx >= len(valid_data[key]):\n",
    "                        # No data available - show empty plot with message\n",
    "                        axs[row, col].text(0.5, 0.5, 'No Data', \n",
    "                                         ha='center', va='center', \n",
    "                                         transform=axs[row, col].transAxes,\n",
    "                                         fontsize=12, color='red')\n",
    "                        continue\n",
    "                    \n",
    "                    # Get data for this model-mode combination\n",
    "                    cam, lidar, cfar, radar = valid_data[key][frame_idx]\n",
    "                    \n",
    "                    # Process radar data\n",
    "                    radar_points = np.load(radar)\n",
    "                    if radar_points.shape[1] == 4:\n",
    "                        radar_points = radar_points[:, :-1]  # Remove speed\n",
    "                    \n",
    "                    radarpc = radar_points.reshape(-1, 3)\n",
    "                    radarpc[:, 1] = -radarpc[:, 1]\n",
    "                    radarpc = data_preparation.transform_point_cloud(\n",
    "                        radarpc,\n",
    "                        [0, 0, -params['azimuth_offset']],\n",
    "                        [-params['x_offset'] / 100, -params['y_offset'] / 100, 0]\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate chamfer distance\n",
    "                    if 'lidarpc_bev' in locals():\n",
    "                        chamfer_dist = compute_chamfer_distance(lidarpc_bev, radarpc)\n",
    "                        chamfer_results[key].append(chamfer_dist)\n",
    "                        \n",
    "                        # Update title with chamfer distance\n",
    "                        base_title = f\"{model}-{mode}\"\n",
    "                        axs[row, col].set_title(f\"{base_title}\\nChamfer: {chamfer_dist:.3f}\", \n",
    "                                              fontsize=10, fontweight='bold')\n",
    "                    \n",
    "                    # Plot BEV for this model-mode\n",
    "                    cb = plot_bev(radarpc, axs[row, col], cax_dict.get((row, col)), False)\n",
    "                    if cb is not None:\n",
    "                        plot_objects.append(cb)\n",
    "            \n",
    "            # Hide empty corner plots\n",
    "            axs[0, 0].axis('off')\n",
    "            axs[2, 0].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.pause(0.01)\n",
    "            Writer.grab_frame()\n",
    "            \n",
    "            # Clear all plot objects\n",
    "            if 'cb_lidar' in locals() and cb_lidar is not None:\n",
    "                cb_lidar.remove()\n",
    "            for cb in plot_objects:\n",
    "                if cb is not None:\n",
    "                    cb.remove()\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Print comprehensive summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    baseline_results = {}  # Store baseline results for comparison\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"\\n{mode.upper()} MODE:\")\n",
    "        print(\"-\" * 40)\n",
    "        mode_results = []\n",
    "        for model in models:\n",
    "            key = f\"{model}-{mode}\"\n",
    "            if key in chamfer_results and chamfer_results[key]:\n",
    "                avg_chamfer = np.mean(chamfer_results[key])\n",
    "                mode_results.append((model, avg_chamfer))\n",
    "                print(f\"{model:10}: {avg_chamfer:.3f}\")\n",
    "                \n",
    "                # Store baseline (resnet18) results\n",
    "                if model == 'resnet18':\n",
    "                    baseline_results[mode] = avg_chamfer\n",
    "            else:\n",
    "                print(f\"{model:10}: No data\")\n",
    "        \n",
    "        if mode_results:\n",
    "            # Find best and worst performers\n",
    "            mode_results.sort(key=lambda x: x[1])\n",
    "            best_model, best_score = mode_results[0]\n",
    "            worst_model, worst_score = mode_results[-1]\n",
    "            print(f\"\\nBest:  {best_model} ({best_score:.3f})\")\n",
    "            print(f\"Worst: {worst_model} ({worst_score:.3f})\")\n",
    "            if len(mode_results) > 1:\n",
    "                improvement = ((worst_score - best_score) / worst_score * 100)\n",
    "                print(f\"Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Add baseline comparison section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON TO BASELINE (ResNet18)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for mode in modes:\n",
    "        if mode not in baseline_results:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{mode.upper()} MODE - Baseline: {baseline_results[mode]:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model in models[1:]:  # Skip resnet18 since it's the baseline\n",
    "            key = f\"{model}-{mode}\"\n",
    "            if key in chamfer_results and chamfer_results[key]:\n",
    "                avg_chamfer = np.mean(chamfer_results[key])\n",
    "                difference = avg_chamfer - baseline_results[mode]\n",
    "                percent_change = (difference / baseline_results[mode]) * 100\n",
    "                \n",
    "                if difference < 0:\n",
    "                    status = \"BETTER\"\n",
    "                    symbol = \"↓\"\n",
    "                    color_indicator = \"✓\"\n",
    "                elif difference > 0:\n",
    "                    status = \"WORSE\"\n",
    "                    symbol = \"↑\"\n",
    "                    color_indicator = \"✗\"\n",
    "                else:\n",
    "                    status = \"SAME\"\n",
    "                    symbol = \"=\"\n",
    "                    color_indicator = \"-\"\n",
    "                \n",
    "                print(f\"{model:10}: {avg_chamfer:.3f} ({symbol}{abs(difference):.3f}, {percent_change:+.1f}%) {color_indicator} {status}\")\n",
    "            else:\n",
    "                print(f\"{model:10}: No data available\")\n",
    "    \n",
    "    # Overall summary across all modes\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OVERALL SUMMARY vs BASELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    overall_baseline = []\n",
    "    overall_comparison = {model: [] for model in models[1:]}\n",
    "    \n",
    "    for mode in modes:\n",
    "        if mode in baseline_results:\n",
    "            overall_baseline.extend(chamfer_results.get(f'resnet18-{mode}', []))\n",
    "            for model in models[1:]:\n",
    "                key = f\"{model}-{mode}\"\n",
    "                if key in chamfer_results:\n",
    "                    overall_comparison[model].extend(chamfer_results[key])\n",
    "    \n",
    "    if overall_baseline:\n",
    "        overall_baseline_avg = np.mean(overall_baseline)\n",
    "        print(f\"Baseline (ResNet18) Overall Average: {overall_baseline_avg:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for model in models[1:]:\n",
    "            if overall_comparison[model]:\n",
    "                model_avg = np.mean(overall_comparison[model])\n",
    "                difference = model_avg - overall_baseline_avg\n",
    "                percent_change = (difference / overall_baseline_avg) * 100\n",
    "                \n",
    "                if difference < 0:\n",
    "                    status = \"BETTER\"\n",
    "                    symbol = \"↓\"\n",
    "                elif difference > 0:\n",
    "                    status = \"WORSE\" \n",
    "                    symbol = \"↑\"\n",
    "                else:\n",
    "                    status = \"SAME\"\n",
    "                    symbol = \"=\"\n",
    "                \n",
    "                print(f\"{model:10}: {model_avg:.3f} ({symbol}{abs(difference):.3f}, {percent_change:+.1f}%) - {status}\")\n",
    "    \n",
    "    print(f\"\\nVideo saved as: comprehensive_model_comparison.mp4\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273e3c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  94%|█████████▍| 305/324 [1:07:43<04:33, 14.39s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  94%|█████████▍| 306/324 [1:07:56<04:10, 13.92s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  95%|█████████▍| 307/324 [1:08:11<04:02, 14.28s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  95%|█████████▌| 308/324 [1:08:25<03:48, 14.25s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  95%|█████████▌| 309/324 [1:08:38<03:28, 13.93s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  96%|█████████▌| 310/324 [1:08:53<03:16, 14.03s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  96%|█████████▌| 311/324 [1:09:06<03:01, 13.94s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  96%|█████████▋| 312/324 [1:09:21<02:48, 14.03s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  97%|█████████▋| 313/324 [1:09:36<02:38, 14.45s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  97%|█████████▋| 314/324 [1:09:51<02:25, 14.50s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  97%|█████████▋| 315/324 [1:10:05<02:10, 14.49s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  98%|█████████▊| 316/324 [1:10:21<01:58, 14.79s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  98%|█████████▊| 317/324 [1:10:35<01:42, 14.69s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  98%|█████████▊| 318/324 [1:10:50<01:28, 14.73s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  98%|█████████▊| 319/324 [1:11:06<01:15, 15.03s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  99%|█████████▉| 320/324 [1:11:20<00:59, 14.88s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  99%|█████████▉| 321/324 [1:11:36<00:45, 15.05s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video:  99%|█████████▉| 322/324 [1:11:52<00:30, 15.41s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video: 100%|█████████▉| 323/324 [1:12:08<00:15, 15.56s/ frames]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating comprehensive comparison video: 100%|██████████| 324/324 [1:12:23<00:00, 13.40s/ frames]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "TRAIN MODE:\n",
      "----------------------------------------\n",
      "resnet18  : 7.696\n",
      "resnet50  : 7.293\n",
      "resnet101 : 7.493\n",
      "resnet152 : 7.358\n",
      "\n",
      "Best:  resnet50 (7.293)\n",
      "Worst: resnet18 (7.696)\n",
      "Improvement: 5.2%\n",
      "\n",
      "TEST MODE:\n",
      "----------------------------------------\n",
      "resnet18  : 1.236\n",
      "resnet50  : 1.210\n",
      "resnet101 : 1.706\n",
      "resnet152 : 2.230\n",
      "\n",
      "Best:  resnet50 (1.210)\n",
      "Worst: resnet152 (2.230)\n",
      "Improvement: 45.7%\n",
      "\n",
      "VAL MODE:\n",
      "----------------------------------------\n",
      "resnet18  : 6.801\n",
      "resnet50  : 6.584\n",
      "resnet101 : 6.879\n",
      "resnet152 : 6.242\n",
      "\n",
      "Best:  resnet152 (6.242)\n",
      "Worst: resnet101 (6.879)\n",
      "Improvement: 9.3%\n",
      "\n",
      "================================================================================\n",
      "COMPARISON TO BASELINE (ResNet18)\n",
      "================================================================================\n",
      "\n",
      "TRAIN MODE - Baseline: 7.696\n",
      "--------------------------------------------------\n",
      "resnet50  : 7.293 (↓0.403, -5.2%) ✓ BETTER\n",
      "resnet101 : 7.493 (↓0.203, -2.6%) ✓ BETTER\n",
      "resnet152 : 7.358 (↓0.338, -4.4%) ✓ BETTER\n",
      "\n",
      "TEST MODE - Baseline: 1.236\n",
      "--------------------------------------------------\n",
      "resnet50  : 1.210 (↓0.026, -2.1%) ✓ BETTER\n",
      "resnet101 : 1.706 (↑0.470, +38.0%) ✗ WORSE\n",
      "resnet152 : 2.230 (↑0.994, +80.4%) ✗ WORSE\n",
      "\n",
      "VAL MODE - Baseline: 6.801\n",
      "--------------------------------------------------\n",
      "resnet50  : 6.584 (↓0.217, -3.2%) ✓ BETTER\n",
      "resnet101 : 6.879 (↑0.078, +1.1%) ✗ WORSE\n",
      "resnet152 : 6.242 (↓0.559, -8.2%) ✓ BETTER\n",
      "\n",
      "==================================================\n",
      "OVERALL SUMMARY vs BASELINE\n",
      "==================================================\n",
      "Baseline (ResNet18) Overall Average: 5.245\n",
      "------------------------------\n",
      "resnet50  : 5.029 (↓0.216, -4.1%) - BETTER\n",
      "resnet101 : 5.360 (↑0.115, +2.2%) - WORSE\n",
      "resnet152 : 5.277 (↑0.032, +0.6%) - WORSE\n",
      "\n",
      "Video saved as: comprehensive_model_comparison.mp4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the video generation with chamfer distance calculation\n",
    "# create_videos_for_all_models_with_chamfer(valid_data, max_frames=20)\n",
    "\n",
    "# Run the comprehensive comparison\n",
    "create_comprehensive_comparison_video(valid_data, max_frames=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riss_experimental_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
